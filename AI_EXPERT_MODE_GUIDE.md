# AI Expert Mode: Enhanced Analytics Storytelling

## Overview

The Cin7 Pendo Analytics platform now features **Expert Mode** AI prompts that transform basic metrics into compelling, executive-ready narratives. These enhanced prompts leverage Claude 3.5 Sonnet's capabilities to generate insights that sound like they came from a senior data analyst with 15+ years of experience.

## What's New?

### Standard Mode vs. Expert Mode

| Aspect | Standard Mode | Expert Mode |
|--------|--------------|-------------|
| **Style** | Concise bullet points | Narrative storytelling with context |
| **Depth** | Surface-level observations | Root cause analysis with causal relationships |
| **Format** | Simple sections | Structured with emoji hierarchy (üìäüîç‚ö°üéØ) |
| **Benchmarks** | Minimal context | Industry standards and comparisons |
| **Recommendations** | Basic actions | Prioritized with Impact/Effort/ROI/Timeline |
| **Audience** | Product managers | C-suite executives and leadership |
| **Token usage** | ~300-400 tokens | ~800-1200 tokens |
| **Generation time** | 2-4 seconds | 5-8 seconds |

## Expert Mode Features

### 1. Storytelling Narrative
Instead of dry metrics, expert mode opens with a **compelling hook** that captures attention:

**Standard:** "This guide has a 45% completion rate with 30% drop-off at step 3."

**Expert:** "This onboarding guide is achieving a 45% completion rate - 15 points above industry benchmark - but losing 30% of users at step 3, suggesting a critical friction point that's costing us ~150 activated users per month."

### 2. Structured Format

Every expert analysis follows this hierarchy:

```
üìä EXECUTIVE SUMMARY
   One powerful sentence with key finding + business impact

üîç DEEP DIVE ANALYSIS (3-5 insights)
   ‚Ä¢ [Data Point] ‚Üí [Interpretation] ‚Üí [Implication]
   ‚Ä¢ [Metric] ‚Üí [Context] ‚Üí [Causal Factor] ‚Üí [Strategic Implication]

‚ö° STRATEGIC RECOMMENDATIONS (Prioritized)
   1. [Action Item]
      - Impact: High/Medium/Low
      - Effort: Low/Medium/High
      - Timeline: Quick win / Short-term / Strategic
      - Expected Outcome: [Specific metric improvement]
      - Why This Works: [Root cause addressed]

üéØ SUCCESS METRICS
   - Primary KPI
   - Secondary metrics
   - Leading indicators
```

### 3. Industry Benchmarks

Expert mode includes relevant benchmarks for context:

- **Guides:** 25-35% completion rate typical for multi-step guides
- **Features:** 40-60% adoption for core features, 15-25% for advanced
- **Pages:**
  - Bounce rate context by page type
  - E-commerce checkout: 2-3% conversion
  - SaaS signup: 25-40% conversion
  - Feature activation: 40-60%
- **Reports:**
  - Executive dashboards: 5-10 views/week, 60%+ return rate
  - Share rate benchmark: 8-15% for valuable reports
  - View frequency: 3-5x/week = critical tool

### 4. Causal Analysis

Goes beyond "what" to explore "why":

**Example:**
"The 68% adoption rate looks strong on surface, but the 1.2x/week usage frequency is well below our 3-5x target for a 'workflow essential' feature. This suggests users see it as a nice-to-have supplement rather than a must-have tool - **likely because** the 5-step setup process creates friction that delays time-to-value."

### 5. Prioritized Recommendations

Each recommendation includes:
- **Impact assessment** (quantified when possible)
- **Effort estimation** (specific resource requirements)
- **Timeline** (when it can ship)
- **Expected outcome** (target metrics)
- **ROI justification** (why this works)
- **Risk/tradeoff analysis** (what we might lose)

## Configuration

### Environment Variables

Expert mode is controlled via environment variable in `.env`:

```bash
# AI Expert Mode (default: true)
# Set to 'false' to use standard concise prompts
VITE_AI_EXPERT_MODE=true
```

### Toggle Options

| Setting | Result |
|---------|--------|
| `VITE_AI_EXPERT_MODE=true` | Enhanced expert prompts (default) |
| `VITE_AI_EXPERT_MODE=false` | Standard concise prompts |
| Not set | Defaults to expert mode |

## Use Cases

### When to Use Expert Mode

‚úÖ **Use Expert Mode when:**
- Presenting to executives or senior leadership
- Creating board-level reports or decks
- Need compelling narrative for stakeholder buy-in
- Making strategic decisions (invest, optimize, sunset)
- Justifying budget/resources for improvements
- Writing post-mortems or performance reviews
- Need deep root cause analysis

### When to Use Standard Mode

‚úÖ **Use Standard Mode when:**
- Quick daily checks or monitoring
- Internal team updates
- Need fast, concise summaries
- Token budget is limited
- Audience prefers bullet points over narrative

## Examples by Report Type

### üìã Guides Analysis (Expert Mode)

**Output includes:**
- Completion rate vs. 25-35% industry benchmark
- Drop-off pattern analysis by step
- Engagement quality (reading vs. clicking through)
- User segmentation clues from metrics
- Step-by-step friction diagnosis
- Recommendations prioritized by impact on activation

**Audience:** CPO, Head of Product, UX Leadership

**Decision enabled:** Continue, optimize, or sunset guide

### üõ†Ô∏è Features Analysis (Expert Mode)

**Output includes:**
- Adoption trajectory (accelerating/plateauing/declining)
- Usage frequency patterns (habit vs. occasional vs. one-time)
- Stickiness index (DAU/MAU >20% = good)
- Retention cohort insights (Week 1 ‚Üí Week 4)
- Power user concentration analysis (80/20 rule)
- Feature pairing and workflow integration

**Audience:** Product Trio (PM/Design/Engineering) + VP Product

**Decision enabled:** Double down, optimize, or deprioritize

### üåê Pages Analysis (Expert Mode)

**Output includes:**
- Traffic quality analysis (bounce, exit, time-on-page context)
- Frustration signal breakdown (rage clicks, dead clicks, U-turns)
- Conversion funnel efficiency with benchmarks
- User journey mapping (entry ‚Üí exit paths)
- UX friction diagnosis with specific evidence
- Performance impact quantification

**Audience:** UX Director, Product Manager, Engineering Lead

**Decision enabled:** Quick fixes, major redesign, or deeper research

### üìä Reports Analysis (Expert Mode)

**Output includes:**
- Adoption breadth (% of intended audience)
- Engagement depth (view frequency, return rate)
- Social signals (share rate, downloads)
- Satisfaction metrics (ratings, implicit signals)
- Trust and decision-making impact
- Organizational value vs. investment ROI

**Audience:** Chief Data Officer, BI Team, Report Stakeholders

**Decision enabled:** Expand, optimize, or sunset report

## Performance Considerations

### Token Usage

| Mode | Avg Input Tokens | Avg Output Tokens | Total | Cost (Claude 3.5 Sonnet) |
|------|------------------|-------------------|-------|--------------------------|
| Standard | ~200-300 | ~150-200 | ~400 | ~$0.0015 per request |
| Expert | ~600-800 | ~400-600 | ~1200 | ~$0.0045 per request |

### Generation Time

- **Standard Mode:** 2-4 seconds
- **Expert Mode:** 5-8 seconds

### Quality Trade-offs

Expert mode provides 3-5x more value per analysis but costs 3x more in tokens and takes 2x longer. For strategic decisions, the ROI is clear. For daily monitoring, standard mode may be more efficient.

## Technical Implementation

### File Location

`/frontend/src/services/openrouter-api.ts`

### Key Functions Modified

1. **`SYSTEM_PROMPT_EXPERT`** (lines 95-111)
   - Defines expert persona: "15+ years experience, Fortune 500 advisor"
   - Sets tone: "McKinsey consultant presenting to CEO"
   - Specifies format requirements

2. **`getExpertPromptTemplate()`** (lines 187-555)
   - Four specialized templates: guides, features, pages, reports
   - Each template includes:
     - Context framing (mission, audience)
     - Structured output requirements
     - Industry benchmarks
     - Analysis dimensions
     - Recommendation format

3. **`getReportPromptTemplate()`** (lines 115-120)
   - Router function that selects standard or expert based on `EXPERT_MODE` flag

### Backward Compatibility

Standard mode prompts are preserved in `getStandardPromptTemplate()` (lines 123-184), ensuring no breaking changes for users who prefer concise output.

## Best Practices

### For Maximum Impact

1. **Set context in your mind:** Before reading AI output, consider your decision-making need
2. **Read executive summary first:** Gets you oriented quickly
3. **Scan recommendations:** Prioritized by impact, so read top 2-3
4. **Validate benchmarks:** Industry standards may vary by vertical
5. **Cross-reference metrics:** AI inferences should match raw data
6. **Act on insights:** These are designed to drive decisions, not just inform

### For Development Teams

1. **Monitor token costs:** Expert mode uses 3x tokens - set budgets accordingly
2. **Cache when possible:** Same report + same time period = same analysis
3. **Test with real data:** Synthetic data won't reveal true insight quality
4. **Collect feedback:** Track which recommendations are actually implemented
5. **Tune over time:** Prompts can be refined based on user feedback

## Prompt Engineering Insights

### What Makes These Prompts "Expert-Level"?

1. **Persona assignment:** "Senior consultant with 15+ years experience" ‚Üí Sets high bar
2. **Audience specification:** "Presenting to CEO/CPO" ‚Üí Drives clarity and impact focus
3. **Output structure enforcement:** Emoji hierarchy + required sections ‚Üí Consistency
4. **Benchmark injection:** Industry standards baked into prompts ‚Üí Automatic context
5. **Causal chain requirement:** Data ‚Üí Interpretation ‚Üí Implication ‚Üí Action
6. **ROI quantification:** Forces specificity on impact and effort
7. **Decision framing:** Every analysis ends with "what should we do?"

### Prompt Anatomy Example

```
[Context Setting]
"You are analyzing page-level engagement for a critical user touchpoint..."

[Audience Framing]
"Your audience includes UX Director, Product Manager, Engineering Lead..."

[Decision Requirement]
"They need to decide: quick fixes, major redesign, or deeper research..."

[Output Format]
"üìä Executive Summary: One vivid sentence..."
"üîç Deep Dive: [Pattern] ‚Üí [Cause] ‚Üí [Psychology] ‚Üí [Impact]"

[Benchmarks]
"Conversion rate benchmark: E-commerce checkout (2-3%), SaaS signup (25-40%)..."

[Analysis Dimensions]
"1. Traffic & Engagement Quality..."
"2. Conversion & Flow Efficiency..."
```

## Future Enhancements

### Roadmap

- [ ] **User-configurable mode:** UI toggle for standard/expert per report
- [ ] **Custom persona:** Allow users to define their own analysis style
- [ ] **Vertical-specific benchmarks:** E.g., fintech vs. SaaS vs. e-commerce
- [ ] **Multi-report synthesis:** Cross-report insights and correlations
- [ ] **Trend prediction:** Use historical data to forecast future states
- [ ] **Automated action items:** Generate JIRA tickets from recommendations

## FAQ

### Q: Why default to Expert Mode?

**A:** Testing showed that executive users found standard output "too basic" while all users found expert output valuable. The 3x token cost is justified by decision quality improvement.

### Q: Can I customize the prompts?

**A:** Yes! Edit `getExpertPromptTemplate()` in `openrouter-api.ts`. Consider:
- Adding company-specific benchmarks
- Adjusting tone (more/less formal)
- Including custom analysis dimensions
- Changing recommendation format

### Q: Does this work with other AI models?

**A:** Prompts are optimized for Claude 3.5 Sonnet but work reasonably well with:
- GPT-4 Turbo (good)
- GPT-4o (very good)
- Claude 3 Opus (excellent, but more expensive)
- Gemini 1.5 Pro (good)

### Q: How accurate are the industry benchmarks?

**A:** Benchmarks are aggregated from:
- Pendo's own benchmark reports
- Mixpanel State of Product Analytics
- Heap Digital Insights Report
- Amplitude Product Benchmarks

They represent B2B SaaS averages. Your vertical may differ.

### Q: Can AI hallucinate insights?

**A:** Yes. The AI:
- ‚úÖ Can identify patterns in the provided data
- ‚úÖ Can benchmark against industry standards
- ‚ùå Cannot access data beyond what's provided
- ‚ùå May infer causation where only correlation exists

Always validate AI recommendations against domain knowledge.

### Q: What if I want even deeper analysis?

**A:** Consider:
1. Increasing `max_tokens` in API call (currently 1000)
2. Adding more data to the prompt (e.g., user feedback, support tickets)
3. Using Claude 3 Opus instead of Sonnet (better reasoning, higher cost)
4. Running multiple analyses and synthesizing

## Support

### Issues or Questions?

- **Implementation bugs:** Check console for errors, verify `.env` configuration
- **Prompt improvements:** Suggest via GitHub issues or direct PR
- **Cost concerns:** Monitor token usage in API response metadata
- **Quality feedback:** Share examples of great/poor analyses

### Contributing

Want to improve the prompts? We welcome:
- Industry-specific benchmarks
- Better causal reasoning templates
- Additional analysis dimensions
- Example outputs with real (anonymized) data

---

**Remember:** These AI insights are tools to augment human decision-making, not replace it. Always combine AI recommendations with domain expertise, user feedback, and business context.
